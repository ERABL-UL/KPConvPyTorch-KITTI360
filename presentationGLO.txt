####################################
########### PROBLÉMATIQUE ##########
####################################

- Malgré que la segmentation et la détection d'objets dans des nuages de points sparses soient deux tâches importantes pour la sécurité en conduite automobile autonome il est encore difficile de le faire.

- Les données disponibles sont, plus souvent qu'autrement, le nerf de la guerre lorsqu'il s'agit d'entraîner un réseau de neurones. Dans le cas présent, il n'y a qu'une quantité très limitée de données annotées. Par contre, un grand jeu de données doit être disponible pour entraîner un réseau en entier. Ce qui oblige, en quelque sorte, d'entraîner un réseau de neurones avec des données sans étiquette et d'utiliser un réseau auto-supervisé/non-supervisé. Les auteurs de l'article précisent qu'ils ont effectué un finetuning avec une petite quantité de données annotées.

- Les réseaux de neurones auto-supervisés les plus performants utilisés sur des nuages de points 3D sont, pour la grande majorité, adaptés pour des scenes à l'intérieur, dans un milieu contrôlé et pour des nuages denses. Lorsqu'appliqués sur des nuages de points extérieurs acquis par un drone où un véhicule en mouvement, la densité varie beaucoup, la variété des objets est plus grandes, les distances entre le capteur et les points sont plus grandes et plus variables, ainsi de suite.

- Dans l'article, on nomme PointContrast qui utilise des nuages de points "pairés" et une list des points qui se correspondent. Cela fonctionne dans les nuages de points denses acquis de façon statique, mais devient beaucoup moins efficace lorsqu'appliqué sur les nuages qui nous intéressent ici (i.e. sparse, extérieur et d'acquisition mobile) parce que les paires de points se font plus rares. On nomme aussi DepthContrast, qui utilise plutôt une seule représentation d'objets par scène... par contre, cette représentation est calculée par global pooling, ce qui entraîne une perte d'information pour les plus petits objets.


####################################
################ BUT ###############
####################################

- Beaucoup d'automobiles à conduite autonome sont munis de capteur d'images et LiDAR, où un fois synchronisés et calibrés, offrent de l'information très riche sur 360 degrés. Étant donné que ces données sont immensément plus faciles à acquérir qu'à annoter, on propose de tirer profit de cette information en distillant un réseau non supervisé pré-entraîné d'images en un réseau 3D. Ce pré-entraînement ne demande pas que les images ni les points ne soient annotés. Il s'agit donc d'une distillation* d'une représentation 2D-vers-3D non supervisée.

* DISTILLATION: Petite parenthèse, dans notre contexte, la distillation réfère à une technique où le modèle apprend de sa propre prédiction/représentation interne. Donc c'est une forme de transfert de savoir où le modèle est entraîné à générer des connaissances supplémentaires pour améliorer sa propre performance. Son utilisation courante consiste à faire un pré-entraînement non supervisé suivi par un fine-tuning supervisé... ce qui est en gros l'approche du papier ici.


####################################
###### DESCRIPTION DES ÉTAPES ######
####################################

APPROCHE:
Pour la création de superpixels...
Les données LiDAR et les images sont synchronisées grâce au données de position de la caméra et du capteur. Les points sont donc projetés dans le frame de la caméra. Plus spécifiquement, on obtient un mapping pc, qu'on voit dans la slide ici, pour chaque caméra c, qui prend en input un point 3D et qui output l'index du pixel 2D correspondant du frame Ic.

Distillation...
fθbck est un réseau de neurones 3D, où θbck sont les paramètres d'entrainement. Le réseau prend en input un nuage de points et sort en output les features de dimensions D pour chaque point. La tâche ici vise a pré-entrainer le réseau sans utiliser d'annotations ou d'étiquettes fait à la main. Donc on utilise les données LiDAR et les images qui ont été alignées et synchronisées préalablement. 
Donc on entraine le réseau de neurones fθbck(·) en alignant les features des points fθbck(P) avec les représentation images pré-entrainé gωbck(I1), . . . , g¯ωbck(IC).  On réussi à faire ça avec un superpixel-driven contrastive-loss... qu'on voit voir tout de suite dans la prochaine slide.


#######################################################
###### DESCRIPTION ET EXPLICATIONS DES RÉSULTATS ######
#######################################################


Le tableau #2 contient des comparaisons concernant les méthodes de pré-entraînement. On compare SLidR avec les baselines de la configuration par linear probing sur nuScenes et la configuration few-shot end-to-end fine-tuning en utilisant 1% des annotations disponibles de nuScenes et SemanticKITTI. 
  - On peut observer qu'absolument toutes les méthodes de pre-training sont meilleurs que l'initialisation aléatoire.
  - PPKT et SLidR, les méthodes basées sur la distillation image-lidar pour le pré-entraînement sont nettement plus performantes que les approches DepthContrast et PointContrast. C'qui met en évidence l'avantage d'exploiter des réseaux auto-supervisés pré-entraînés à partir d'images pour l'apprentissage de représentations 3D Lidar, comme préconisé ici dans cet article. 
  - Finalement, SLidR a obtenu des meilleures performances de segmentation sémantique, en particulier sur la configuration par linear probing. Ca démontre que cette méthode là qui est basée sur les superpixels est supérieure.


Les résulats finaux sont présenté dans le tableau #4. On peut remarquer que SLidR apporte une amélioration significative de 2.3 points en mAP par rapport à une situation où aucun pré-entraînement a été effectué. SLidR surpasse aussi PPKT avec un gain de 1.2 et 1.3 point à 10% et 20% de données annotées respectivement pour le mean average precision (mAP).


####################################
########### Contribution ###########
####################################


La clé de la méthode présentée est l'utilisation de superpixels pour produire une représentation par points qui est "object-aware" et qui est adaptée, par exemple, à la segmentation sémantique et à la détection d'objets. L'article montre que SLidR produit des très bonnes représentations de nuages de points qui se transfèrent et se généralisent bien à plusieurs tâches et à plusieurs ensembles de données. La méthode surpasse même l'état de l'art.


####################################
############ Conclusion ############
####################################

- Améliorations à apporter
Une première limitation peut survenir dans des conditions de faible luminosité car les superpixels calculés peuvent fournir des segments d'objets non d'objets non pertinents, ce qui nuit aux performances de notre méthode

Une autre limitation survient lorsque les caractéristiques de l'image de sortie sont similaires entre deux superpixels. tures de l'image de sortie sont similaires entre deux superpixels. Donc, la contrastive loss va essayer d'imposer une solution où le feature de superpoint fc1 est corrélé à gc1 mais non corrélé à gc2, ce qui est en fait impossible.



